{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b10a9ad0-31f8-46c0-b516-c5f3e198d2b3",
   "metadata": {},
   "source": [
    "## Math 475 Final Project\n",
    "\n",
    "#### Data Exploration and Preprocessing\n",
    "\n",
    "Bryson Herron"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "073145c9-32be-4dda-9c1d-553636860556",
   "metadata": {},
   "source": [
    "#### Neccessary imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ebc6412-3675-4235-aa84-a8700a37be88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57183a2a-80bb-4b5a-94cc-ba916a536539",
   "metadata": {},
   "source": [
    "#### Loading Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "efd1f99f-e6c0-4ee5-b301-f09bef4317ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List each JSON file individually\n",
    "file_names = ['Streaming_History_Audio_2021-2023_0.json', 'Streaming_History_Audio_2023-2024_1.json', 'Streaming_History_Audio_2024_2.json']\n",
    "\n",
    "# Initialize an empty list to store DataFrames\n",
    "data_frames = []\n",
    "\n",
    "# Loop through each specified file and load it into a DataFrame\n",
    "for file in file_names:\n",
    "    try:\n",
    "        df = pd.read_json(file)\n",
    "        data_frames.append(df)\n",
    "    except ValueError as e:\n",
    "        print(f\"Could not read {file}: {e}\")\n",
    "\n",
    "# Concatenate all the DataFrames into a single DataFrame\n",
    "if data_frames:\n",
    "    spotify_data = pd.concat(data_frames, ignore_index=True)\n",
    "else:\n",
    "    print(\"No data frames to concatenate. Check the JSON file contents or file names.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf45413-0757-407f-a85f-bd885adca75f",
   "metadata": {},
   "source": [
    "My spotify data set contains 42894 song entries. Each entry contains the username of the account listened, personal account identifyiers, song name, time played, album name, album artist name, and information on how the song was selected and why it ended. There are also additional columns that are used when podcasts are included, but I have chosen to leave out my podcast data for this project as I want to focus on the songs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a341563f-88fd-46de-ab41-6f2d6497950f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42894, 21)\n",
      "\n",
      "Index(['ts', 'username', 'platform', 'ms_played', 'conn_country',\n",
      "       'ip_addr_decrypted', 'user_agent_decrypted',\n",
      "       'master_metadata_track_name', 'master_metadata_album_artist_name',\n",
      "       'master_metadata_album_album_name', 'spotify_track_uri', 'episode_name',\n",
      "       'episode_show_name', 'spotify_episode_uri', 'reason_start',\n",
      "       'reason_end', 'shuffle', 'skipped', 'offline', 'offline_timestamp',\n",
      "       'incognito_mode'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(spotify_data.shape)\n",
    "print()\n",
    "print(spotify_data.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43edfdbb-3369-4e32-b5d4-ce7f88bcd45c",
   "metadata": {},
   "source": [
    "#### EDA:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6707be2b-f6a5-448f-9218-e274be7230ff",
   "metadata": {},
   "source": [
    "I found that in addition to useful song data, there is also many columns for spotify user information such as IP, connected country, ect... These will be removed in data processing as they are not useful to the prediction algorithm. I also am printing off the total number of null values, as well as which column they are in. This helps when deciding which method to use for removing them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d20fd0c9-d715-4dc7-a634-a124a01c055e",
   "metadata": {},
   "source": [
    "#### Finding null values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea682711-33db-4a70-b2cf-54b812de4d04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of null values in each column:\n",
      "ts                                       0\n",
      "username                                 0\n",
      "platform                                 0\n",
      "ms_played                                0\n",
      "conn_country                             0\n",
      "ip_addr_decrypted                        0\n",
      "user_agent_decrypted                  7532\n",
      "master_metadata_track_name             145\n",
      "master_metadata_album_artist_name      145\n",
      "master_metadata_album_album_name       145\n",
      "spotify_track_uri                      145\n",
      "episode_name                         42751\n",
      "episode_show_name                    42751\n",
      "spotify_episode_uri                  42751\n",
      "reason_start                             0\n",
      "reason_end                               0\n",
      "shuffle                                  0\n",
      "skipped                              13116\n",
      "offline                                  0\n",
      "offline_timestamp                        0\n",
      "incognito_mode                           0\n",
      "dtype: int64\n",
      "\n",
      "Columns with null values:\n",
      "user_agent_decrypted                  7532\n",
      "master_metadata_track_name             145\n",
      "master_metadata_album_artist_name      145\n",
      "master_metadata_album_album_name       145\n",
      "spotify_track_uri                      145\n",
      "episode_name                         42751\n",
      "episode_show_name                    42751\n",
      "spotify_episode_uri                  42751\n",
      "skipped                              13116\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check for null values in the DataFrame\n",
    "null_counts = spotify_data.isnull().sum()\n",
    "\n",
    "# Filter columns with null values\n",
    "columns_with_nulls = null_counts[null_counts > 0]\n",
    "\n",
    "# Print the results\n",
    "print(\"Number of null values in each column:\")\n",
    "print(null_counts)\n",
    "\n",
    "print(\"\\nColumns with null values:\")\n",
    "if columns_with_nulls.empty:\n",
    "    print(\"No columns with null values.\")\n",
    "else:\n",
    "    print(columns_with_nulls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52e4acd-3e8c-4c1e-914a-24231407e27b",
   "metadata": {},
   "source": [
    "Note that most of the null/NaN values are from personal data/information that will be removed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc4df38a-a7d2-469c-9584-c78b9a967a15",
   "metadata": {},
   "source": [
    "#### Data cleaning:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d04ec19-24ca-4b7b-9429-6e9d46cb988b",
   "metadata": {},
   "source": [
    "This is where I am removing the uneccesarry user information. I am also encoding all non-numerical columns with two different methods. Frequency encoding for song names, album names, and album artist. Since there are a very large number of unique values for these columns I feel this is the best approach. I am then using Binary encoding for reason start, and reason end as they only contain 8 values each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a6faa13-ffc6-4466-9b75-8baca7b52503",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean DataFrame to remove columns with excessive nulls / irrelevant to this project\n",
    "spotify_data = spotify_data.drop(columns=['platform', 'ts', 'username', 'user_agent_decrypted', 'episode_name','episode_show_name','spotify_episode_uri', 'offline', 'offline_timestamp', 'incognito_mode', 'ip_addr_decrypted', 'conn_country'])\n",
    "\n",
    "# Convert 'ms_played' from milliseconds to seconds\n",
    "spotify_data['ms_played'] = spotify_data['ms_played'] / 1000\n",
    "\n",
    "# Rename the column to 'seconds_played'\n",
    "spotify_data.rename(columns={'ms_played': 'seconds_played'}, inplace=True)\n",
    "\n",
    "# Convert NaN values in 'skipped' column to 0 and non-NaN values to 1\n",
    "spotify_data['skipped'] = spotify_data['skipped'].notna().astype(int)\n",
    "\n",
    "# Frequency encoding for names of songs albums and artists\n",
    "spotify_data['song_freq'] = spotify_data['master_metadata_track_name'].map(spotify_data['master_metadata_track_name'].value_counts())\n",
    "spotify_data['album_freq'] = spotify_data['master_metadata_album_album_name'].map(spotify_data['master_metadata_album_album_name'].value_counts())\n",
    "spotify_data['album_freq_artist'] = spotify_data['master_metadata_album_artist_name'].map(spotify_data['master_metadata_album_artist_name'].value_counts())\n",
    "\n",
    "# Binary encoding for 'reason_start' and 'reason_end'\n",
    "spotify_data = pd.get_dummies(spotify_data, columns=['reason_start', 'reason_end'], drop_first=True)\n",
    "\n",
    "# Handle NaNs\n",
    "spotify_data.fillna(0, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c47c7f-fc65-4d37-9a66-abb56d395262",
   "metadata": {},
   "source": [
    "#### Recount nulls:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d4edfa-a260-4e1b-82cd-4135a6fa147c",
   "metadata": {},
   "source": [
    "Many of the columns containing nulls were the columns I was already removing for being unhelpful, so this cleared a large number of them. For the remaining nulls I used fillna and set them to 0. I felt this was the best option for the dataset as the total number of nulls was relativily small, and setting them to zero would not cause errors further down when performing feature selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "843b1104-f363-4c91-b1c3-c35ba6af8293",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of null values in each column:\n",
      "seconds_played                             0\n",
      "master_metadata_track_name                 0\n",
      "master_metadata_album_artist_name          0\n",
      "master_metadata_album_album_name           0\n",
      "spotify_track_uri                          0\n",
      "shuffle                                    0\n",
      "skipped                                    0\n",
      "song_freq                                  0\n",
      "album_freq                                 0\n",
      "album_freq_artist                          0\n",
      "reason_start_backbtn                       0\n",
      "reason_start_clickrow                      0\n",
      "reason_start_fwdbtn                        0\n",
      "reason_start_playbtn                       0\n",
      "reason_start_remote                        0\n",
      "reason_start_trackdone                     0\n",
      "reason_start_trackerror                    0\n",
      "reason_start_unknown                       0\n",
      "reason_end_endplay                         0\n",
      "reason_end_fwdbtn                          0\n",
      "reason_end_logout                          0\n",
      "reason_end_remote                          0\n",
      "reason_end_trackdone                       0\n",
      "reason_end_trackerror                      0\n",
      "reason_end_unexpected-exit                 0\n",
      "reason_end_unexpected-exit-while-paused    0\n",
      "reason_end_unknown                         0\n",
      "dtype: int64\n",
      "\n",
      "Columns with null values:\n",
      "No columns with null values.\n"
     ]
    }
   ],
   "source": [
    "# Check for null values in the DataFrame\n",
    "null_counts = spotify_data.isnull().sum()\n",
    "\n",
    "# Filter columns with null values\n",
    "columns_with_nulls = null_counts[null_counts > 0]\n",
    "\n",
    "# Print the results\n",
    "print(\"Number of null values in each column:\")\n",
    "print(null_counts)\n",
    "\n",
    "print(\"\\nColumns with null values:\")\n",
    "if columns_with_nulls.empty:\n",
    "    print(\"No columns with null values.\")\n",
    "else:\n",
    "    print(columns_with_nulls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe3dc3d-5556-42bd-a895-1b47f0e9437c",
   "metadata": {},
   "source": [
    "#### Split data into training and test sets then scale it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "754280da-9a6c-4923-8301-646bb346473e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define X as a list that contains all features except target\n",
    "x = spotify_data.drop(['seconds_played', 'spotify_track_uri', 'master_metadata_track_name', 'master_metadata_album_album_name', 'master_metadata_album_artist_name'], axis=1)\n",
    "\n",
    "# Define y as seconds played\n",
    "y = spotify_data['seconds_played']\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit and transform the training data\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "\n",
    "# Transform the test data (using the same scaler fitted on the training data)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f50b2aea-884f-4079-bba5-526c9ce6f5b7",
   "metadata": {},
   "source": [
    "#### Feature Selection Using a RandomForestGregressor model:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746215cf-bed6-4d9e-a79f-70a3492adda4",
   "metadata": {},
   "source": [
    "This code trains a Random Forest regression model using the RandomForestRegressor from scikit-learn with 500 estimators and a fixed random seed. I felt this was a good approach as many of the features would have a non-linear relationship to the target variable and this model is well suited to handle this scenario. After training the model on the training data (X_train and y_train), it calculates the importance of each feature, sorts them in descending order, and outputs the top 5 most important features. Finally, my model makes predictions on the test set (X_test), and the R² score is calculated to evaluate its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "406c7a47-81f9-4699-96db-9d58bf73d695",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 features: ['song_freq', 'album_freq_artist', 'album_freq', 'reason_end_trackdone', 'skipped']\n"
     ]
    }
   ],
   "source": [
    "# Initialize the Random Forest model\n",
    "rf_model = RandomForestRegressor(n_estimators=500, random_state=42)  # Note that the 500 estimators do take a fair amount of time and computing power\n",
    "\n",
    "# Train the model\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Get feature importances\n",
    "importances = rf_model.feature_importances_\n",
    "features = x.columns\n",
    "\n",
    "# Create a DataFrame with feature names and their importance\n",
    "importance_df = pd.DataFrame({'Feature': features, 'Importance': importances})\n",
    "\n",
    "# Sort by importance and select the top 5\n",
    "top_features = importance_df.sort_values(by='Importance', ascending=False).head(5)\n",
    "\n",
    "# Extract top 5 feature names as a list\n",
    "top_feature_names = top_features['Feature'].tolist()\n",
    "print(\"Top 5 features:\", top_feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5a082e-afe6-4891-97d7-316fc41f5174",
   "metadata": {},
   "source": [
    "#### Evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7889b981-1ff1-4ca9-84bc-25bbd1247d70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R² Score: 0.5788891971957113\n"
     ]
    }
   ],
   "source": [
    "# Predict and evaluate\n",
    "y_pred = rf_model.predict(X_test)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f\"R² Score: {r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e375329b-84eb-433f-be84-b9dfcde265ce",
   "metadata": {},
   "source": [
    "The result I achieved is an R^2 score of 0.5789. While this is not a very high score, I feel that it is fair for this dataset due to one major reason. The overall biggest factor for determining how long a song will play is the mood of the person listening to it. Some days a song will come on and be listened to in its entirety, then another day the listener could be in a different mood and skip it right away. There are a large number of external factors when it comes to listening to music that you could never hope to quantify/categroize. It is for these reasons that I feel an r^2 score of 0.5789 is very good for a model of this size and complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90225231-977b-4fff-8706-1925b925282a",
   "metadata": {},
   "source": [
    "#### Challenges to overcome:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d3984d-834f-490b-a24b-efc176422da5",
   "metadata": {},
   "source": [
    "        The primary challenge for this data set was the non-measurable factors. I touched on this above but one of these factors is listener mood. The general mood of the listener is the single biggest factor when it comes to how long a song will be listened to. Not only can this not be measured through simple data collection and analysis, but it would also be tough to quantify. Due to factors like this, it is impossible to make a fully accurate model for this dataset. For this project, I decided to do the best I could with what I had to try and mitigate this as much as possible.\r",
    "    \n",
    "    Another issue that arose was the very high computational cost of this model. For the final model, I had to significantly modify and simplify it for submission. The initial most accurate model took over 20 minutes for my computer to train. I found this unacceptable as any minor code changes would require me to re-train the model and wait another 20 minutes to test. To resolve this I lowered the number of estimators to 500 and removed some extra testing. This did lower the overall accuracy from around 63% to 57.9% but for the amount of time saved, I considered this a much better alternative.    \r\n",
    "    One final issue that was fairly easy to overcome was a large amount of unnecessary data. Since I imported this data directly from my own Spotify account, there were a lot of personal identifiers, OS information, and other unneeded data. There was also a lot of non-numerical data that needed to be encoded    .\r\n",
    "    If I had more time with this project I would experiment with different model types and features to try and achieve an even better balance of accuracy vs. computational demand.\r\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d83bd3a-d6e8-45cf-9724-c81572a9441b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
