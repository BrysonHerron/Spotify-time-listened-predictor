# Math-475 Final Project
## Bryson Herron

- For this project, I wanted to do something that was personalized to myself but that I could also use outside of this class. I decided to file a data request with Spotify and do a model prediction on the data of my listening history. I decided on this for a few reasons: I wanted to do something unique, it contains elements from many of our class topics, and I also just think that it’s really interesting. 
- Starting out, I immediately ran into some issues. After importing the JSON files and creating one large data frame, I performed EDA. I found that my data set contains 42894 song entries. Each entry contains the username of the account listened, personal account identifiers, song name, time played, album name, album artist name, and information on how the song was selected and why it ended. There are also additional columns that are used when podcasts are included, but I chose to leave out my podcast data for this project as I wanted to focus on the songs. I also found that in addition to useful song data, there are also many columns for Spotify user information such as IP, connected country, etc... These will be removed in data processing as they are not useful to the prediction algorithm. I also printed off the total number of null values, as well as which column they are in. This helped when deciding which method to use when removing them.
- After performing my EDA I found a very large number of null values. Luckily most of them were in columns that I was planning to remove anyway as they do not contribute to time listened to. These are the user information columns mentioned above. I also found what appeared to be 145 corrupted songs. These entries were removed entirely during data processing. The data cleaning stage was where I removed all unnecessary user information. I also encoded all non-numerical columns through two different methods. I used frequency encoding for song names, album names, and album artists. Since there are a very large number of unique values for these columns, I felt this was the best approach. I then used Binary encoding for reason start, and reason end as they only contain 8 values each. Now performing a new recount of the null values showed that there were 0 remaining columns with nulls in them.
- I had a hard time choosing a model for this project. I tried many different model types, and each one either suffered from extremely low accuracy or unrealistic computational cost. I finally settled on using a RandomForrestRegressor model. I felt this was a good approach as many of the features have a non-linear relationship to the target variable and this model is well suited to handle this scenario. After training the model on the training data (X_train and y_train), it calculated the importance of each feature, sorted them in descending order, and output the top 5 most important features. Finally, my model made its predictions on the test set (X_test), and the R² score was then calculated to evaluate its performance.
- During my evaluation of the model, I achieved an R^2 score of 0.5789. While this is not a very high score, I feel that it is fair for this dataset due to one major reason. The overall biggest factor for determining how long a song will play is the mood of the person listening to it. Some days a song will come on and be listened to in its entirety, then another day the listener could be in a different mood and skip it right away. There are many external factors when it comes to listening to music that you could never hope to quantify/categorize. It is for these reasons that I feel an r^2 score of 0.5789 is very good for a model of this size and complexity.
- The primary challenge for this data set was the non-measurable factors. I touched on this above but one of these factors is listener mood. The general mood of the listener is the single biggest factor when it comes to how long a song will be listened to. Not only can this not be measured through simple data collection and analysis, but it would also be tough to quantify. Due to factors like this, it is impossible to make a fully accurate model for this dataset. For this project, I decided to do the best I could with what I had to try and mitigate this as much as possible.
- Another issue that arose was the very high computational cost of this model. For the final model, I had to significantly modify and simplify it for submission. The initial most accurate model took over 20 minutes for my computer to train. I found this unacceptable as any minor code changes would require me to re-train the model and wait another 20 minutes to test. To resolve this, I lowered the number of estimators to 500 and removed some extra testing. This did lower the overall accuracy from around 63% to 57.9% but for time saved, I considered this a much better alternative. One final issue that was easy to overcome was a large amount of unnecessary data. Since I imported this data directly from my own Spotify account, there were a lot of personal identifiers, OS information, and other unneeded data. There was also a lot of non-numerical data that needed to be encoded. If I had more time with this project, I would experiment with even more model types and features to try and achieve an even better balance of accuracy vs. computational demand.
- Overall, I am very pleased with how this project turned out. The amount of time a user will spend listening to a song is something that turned out to be very hard to predict. Despite this, I was a model that was able to account for over 57% of the variability.

